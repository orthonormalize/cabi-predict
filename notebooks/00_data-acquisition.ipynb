{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose:\n",
    "    # Acquire data that could be useful for bikeshare prediction task\n",
    "    # Store to local DB\n",
    "    \n",
    "# Design considerations:\n",
    "    # Allow periodic updates\n",
    "    # Otherwise, modify input stream as little as possible\n",
    "    # Robustness + Adaptability >> Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following data could be useful:\n",
    "\n",
    "A. Historical (for Training)\n",
    "1. CaBi Trip History\n",
    "2. CaBi Dock Status History\n",
    "3. Weather History\n",
    "4. Calendar History (weekdays/weekends, holiday schedules, etc.)\n",
    "\n",
    "B. Real-Time (for Inference)\n",
    "1. CaBi Trips (not available realtime, but ...)\n",
    "2. CaBi Dock Status\n",
    "3. Weather\n",
    "4. Calendar/Clock (date, time, day_of_week, is_holiday, etc.)\n",
    "\n",
    "C. Static \n",
    "1. Station geodata (lat/lon/elev)\n",
    "2. Political boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import datetime\n",
    "import calendar\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "import re\n",
    "import holidays\n",
    "import zipfile\n",
    "import boto3\n",
    "from botocore.handlers import disable_signing\n",
    "\n",
    "cabi_start_date = datetime.datetime(2010,9,19,0,0)\n",
    "date_format_standard = '%Y-%m-%d %H:%M:%S'\n",
    "max_days_request_cabitracker = 7\n",
    "sleep_time_cabitracker = 5 # seconds\n",
    "\n",
    "raw_data_dir = r'C:\\Users\\rek\\Desktop\\GH\\cabi-predict\\data\\0_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables(db):\n",
    "    table_names=[]\n",
    "    conn = sqlite3.connect(db)\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        table_names = [ttt[0] for ttt in tables]\n",
    "        conn.close()\n",
    "    except:\n",
    "        conn.close()\n",
    "    return(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_recent_timestamp(db,table,datecol):\n",
    "    '''\n",
    "    Return most recent timestamp (column: datecol) in table of database db\n",
    "    This function assumes all timestamps in db are already strings written in date_format_standard\n",
    "    If table does not exist, return cabi_start_date\n",
    "    '''\n",
    "    if (isinstance(datecol,list)):\n",
    "        datecol=datecol[0] # if inconsistent datecol name, use the single col resulting from preproc\n",
    "    table_names = get_tables(db)\n",
    "    if (table not in table_names):\n",
    "        return(cabi_start_date)\n",
    "    conn=sqlite3.connect(db)\n",
    "    try:\n",
    "        query = \"select \"+datecol+\" from \"+table\\\n",
    "                    +\" WHERE \"+datecol+\">'\"+cabi_start_date.strftime('%Y-%m-%d')+\"'\"\\\n",
    "                    +\" ORDER BY \"+datecol+\" DESC LIMIT 1\"  # ++: CREATE INDEX?\n",
    "        date_df = pd.read_sql_query(query, conn)\n",
    "        most_recent = datetime.datetime.strptime(date_df.to_numpy()[0][0],'%Y-%m-%d %H:%M:%S')\\\n",
    "                        if len(date_df) else cabi_start_date\n",
    "        most_recent = max(cabi_start_date,most_recent)\n",
    "        conn.close()\n",
    "        return(most_recent)\n",
    "    except:\n",
    "        conn.close()\n",
    "        print(\"SQL Query did not work, returning cabi_start_date as most_recent\")\n",
    "        return(cabi_start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CaBi Trip History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.capitalbikeshare.com/system-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trip_history(db,tname,mrt,kw):\n",
    "    \"\"\"\n",
    "    Retrieve new trip_history files from Capital Bikeshare's System Data S3 Bucket\n",
    "    https://www.capitalbikeshare.com/system-data\n",
    "    Determine whether to read each item in bucket by:\n",
    "        parse filename to find string identifying YYYY or YYYYMM\n",
    "        compare that date vs. most recent timestamp mrt.\n",
    "        Read new months only.\n",
    "    \"\"\"\n",
    "    \n",
    "    # for each file available at S3:\n",
    "        # is it a zip? If so, interpret Y/YM\n",
    "        # filter: if zip and Y/M after mrt, download the zip\n",
    "        # get csv filenames inside zip, containing no slashes\n",
    "        # read_csv\n",
    "    # return naive pd.concat(all newly read dataframes)\n",
    "    os.makedirs(raw_data_dir,exist_ok=True)\n",
    "    cabi_bucket_name = 'capitalbikeshare-data'\n",
    "    s3resource = boto3.resource('s3')\n",
    "    s3resource.meta.client.meta.events.register('choose-signer.s3.*', disable_signing)\n",
    "    s3bucket = s3resource.Bucket(cabi_bucket_name)\n",
    "    dfs = [pd.DataFrame()] # initz w/ empty DF \n",
    "    for obj in s3bucket.objects.all():\n",
    "        if (os.path.splitext(obj.key) and (os.path.splitext(obj.key)[-1].lower()=='.zip')):\n",
    "            zname = os.path.split(obj.key)[-1]\n",
    "            YM_cand = [s for s in zname.split('-') if ((s.isdigit()) and (s[:2]=='20'))]\n",
    "            assert(len(YM_cand)<2)\n",
    "            if (YM_cand):\n",
    "                Y=int(YM_cand[0][:4])\n",
    "                M=int(YM_cand[0][4:] or '12')\n",
    "                if (datetime.datetime(Y,M,15) > mrt): # hack assumes each month comes at once\n",
    "                    print('Reading file %s...' % obj.key)\n",
    "                    targetzipfile=os.path.join(raw_data_dir,obj.key)\n",
    "                    s3bucket.download_file(obj.key, targetzipfile)\n",
    "                    zfile = zipfile.ZipFile(open(targetzipfile,'rb'))\n",
    "                    csvs = [x for x in zfile.namelist() if (('/' not in x))] \n",
    "                    # stopped checking extension b/c July 2019 (356645 missing rows)\n",
    "                    for c in csvs:\n",
    "                        dfs.append(pd.read_csv(io.BytesIO(zfile.read(c))))\n",
    "                else:\n",
    "                    print('Skipping file %s, which has presumably been read previously' % obj.key)\n",
    "            else:\n",
    "                print('Skipping file %s, cannot identify date string in filename' % obj.key)\n",
    "        else:\n",
    "            print('Skipping file %s because it is not a zip' % obj.key)\n",
    "    return(pd.concat(dfs,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenges:\n",
    "    # Zip files are inconsistent\n",
    "        # Some contain multiple CSVs\n",
    "        # Some contain CSVs without the extension '.csv'\n",
    "        # CSV format changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options:\n",
    "# ignore_keys: don't ever download these filenames\n",
    "# archive_collisions:\n",
    "    # if file exists, only DL if new file size. Archive the old version\n",
    "# verbose: print action for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Real-Time Trips**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anonymization => real-time trip info is limited.\n",
    "\n",
    "\n",
    "Cannot see start and end points of the same trip until TH is published the following month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outage Data (cabitracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CabiTracker: http://cabitracker.com/\n",
    "# Daniel Gohlke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_current_outages(db,tname,mrt,kw):\n",
    "    # This table should always be written with if_exists='replace'\n",
    "        # current_outages get converted to past_outages as soon as they're completed\n",
    "    uri_cto_current = r'http://cabitracker.com/status.php?format=json'\n",
    "    tic=time.time()\n",
    "    print('Reading Current Outage data from CabiTracker...')\n",
    "    cto_raw = requests.get(uri_cto_current)\n",
    "    print('Execution Time(s): %.2f'%(time.time()-tic))\n",
    "    timestamp = datetime.datetime.now().strftime(date_format_standard)\n",
    "    df_cto0 = pd.read_json(io.BytesIO(cto_raw.content))\n",
    "    return(pd.concat([pd.Series(timestamp,index=df_cto0.index,name='timestamp_now'),df_cto0],axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_past_outages(db,tname,mrt,kw):    \n",
    "    \"\"\"\n",
    "    Retrieve dock outage data from cabitracker\n",
    "    start_date, end_date = datetime.dates (inclusive) defining time period\n",
    "    cabitracker retrieves outages based on the \"End\" time of each outage\n",
    "    per_month => int, divide each month into # date-ranges, hack to prevent burnout requests\n",
    "    refresh_all => bool. If True, read all data anew.\n",
    "                        If False, reset start_date to date of most recent DB entry. \n",
    "            (if one is trying to recover any missing data before that, must use refresh_all=True with specific dates)\n",
    "    \"\"\"\n",
    "    uri_cto_past_gen = r'http://cabitracker.com/downloadoutage.php?s=%s&e=%s'\n",
    "           # two parameters for uri: s=startdate and e=enddate\n",
    "           # request will fail if too much data, but we cannot probe its length before reading!\n",
    "           # safety: just request a small amount at a time.\n",
    "           # 7-day request period has worked fine through eo2020 data.\n",
    "                # but could easily fail if outages modestly increase in future\n",
    "                # 15-day request period failed for ~10% of spring months\n",
    "                \n",
    "    start_date = max(mrt.date(),datetime.date(2016,10,1)) # first date avail at cabitracker is 2016-10-01\n",
    "    end_date = datetime.date.today()\n",
    "    total_days = 1 + (end_date-start_date).days\n",
    "    dfs = []\n",
    "    for j in range(0,total_days,max_days_request_cabitracker):\n",
    "        d0 = start_date + datetime.timedelta(days=j)\n",
    "        d1 = start_date + datetime.timedelta(days=j+max_days_request_cabitracker-1)\n",
    "        d1 = min(d1,end_date)\n",
    "        (s0,s1) = (d0.strftime('%Y-%m-%d'),d1.strftime('%Y-%m-%d'))\n",
    "        try:\n",
    "            uri = uri_cto_past_gen % (s0,s1)\n",
    "            print('Retrieving cabitracker outage data between %s and %s ...' % (s0,s1))\n",
    "            outages_object = requests.get(uri)\n",
    "            dfs.append(pd.read_csv(io.BytesIO(outages_object.content)))\n",
    "        except:\n",
    "            print('       Failed to process data between %s and %s !!!' % (s0,s1))\n",
    "        time.sleep(5)\n",
    "    return(pd.concat(dfs,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Outages only move from status.php to outage_history.php upon resolution.\n",
    "\n",
    "# To see real-time dock status at any past timestamp, need to check both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These two data streams are a bit different\n",
    "# Past outages include qualifiers (near, releasable/dockable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Crossing:\n",
    "\n",
    "# https://www.visualcrossing.com\n",
    "# https://www.visualcrossing.com/resources/documentation/weather-api/how-do-i-get-started-with-the-weather-api/\n",
    "# https://www.visualcrossing.com/resources/documentation/weather-api/weather-api-documentation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_weather(db,tname,mrt,kw):\n",
    "    vc_url_base = r'https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?'\n",
    "    weather_dateformat_request = '%Y-%m-%dT%H:%M:%S'\n",
    "    print(mrt)\n",
    "    weather_dateformat_rawreturn = kw['raw_date_formats']['weather']['datetime']\n",
    "    startdatetime = mrt + datetime.timedelta(minutes=30)\n",
    "    enddatetime = kw['now']\n",
    "    vc_file = r'C:\\Users\\rek\\.credentials\\vc.txt'\n",
    "    with open(vc_file) as f:\n",
    "        key_vc = f.read()\n",
    "    d_vc = {'goal':'history',\n",
    "            'startDateTime':startdatetime.strftime(weather_dateformat_request),\n",
    "           'endDateTime':enddatetime.strftime(weather_dateformat_request),\n",
    "           'aggregateHours':str(1),\n",
    "           'contentType':'csv',\n",
    "           'unitGroup':'us',\n",
    "           'locations':'Washington,DC',\n",
    "           'key':key_vc,\n",
    "            'includeAstronomy':'true',\n",
    "            'shortColumnNames':'true',\n",
    "            'collectStationContribution':'true'\n",
    "           }\n",
    "    query_parameters = ('&'.join(('%s=%s' % (k,d_vc[k])) for k in d_vc))\n",
    "    vc_url_full = vc_url_base + query_parameters\n",
    "    print(startdatetime)\n",
    "    print(enddatetime)\n",
    "    print(vc_url_full[:170])\n",
    "    #assert False, \"stopping here\"\n",
    "    tic=time.time()\n",
    "    print('Reading new Weather Data from Visual Crossing...')\n",
    "    vcr_raw = requests.get(vc_url_full)    # check for cost b4 running. full set is $8.00\n",
    "    print('Execution Time(s): %.2f'%(time.time()-tic))\n",
    "    dfw_raw = pd.read_csv(io.BytesIO(vcr_raw.content))\n",
    "    dfw_raw.to_json(path_or_buf='weather_backup_'+enddatetime.strftime('%Y%m%d %H%M%S')+'.json')\n",
    "    return(dfw_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/holidays/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_holidays(db,tname,mrt,kw):\n",
    "    # holidays is a very small table. Easy to just get this one anew every update.\n",
    "    h_dc = (holidays.US(state='DC',years=range(mrt.year,5+kw['now'].year)))\n",
    "    df_holidays = pd.DataFrame.from_dict(h_dc,orient='index',columns=['hol_name'])\\\n",
    "                .reset_index().rename(columns={'index':'date'})\n",
    "    return(df_holidays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_update(db,tables=None,startover=False,startover_freeonly=True,now=None):\n",
    "    '''\n",
    "    Update the cabi database with newest data\n",
    "    Each table in input db is presumed to be accurate & complete up to its most recent entry.\n",
    "    This function read_data_update will find any newer data and append it.\n",
    "    Preserve Raw data format, except for::\n",
    "        1) field names => .lower().replace(' ','_')\n",
    "        2) all dates/datetimes stored in DB will be converted to a standardized string format\n",
    "    \n",
    "    db: file path to database where raw data is and/or will be stored\n",
    "    tables: dict. Keys should match defaults below. Allows custom table names, if desired\n",
    "    startover: ignore all existing data and read everything from scratch. Default False (update only)\n",
    "    startover_freeonly: even if starting over, refrain from re-accessing data behind paywalls.\n",
    "            Default True (so that weather data can be preserved even if starting over everything else)\n",
    "    No return value\n",
    "    '''\n",
    "    default_tables = {k:k for k in \\\n",
    "                      ['trip_history','past_outages','current_outages','weather','holidays']}\n",
    "    default_tables.update(tables or {})\n",
    "    tables=default_tables\n",
    "    kw={}\n",
    "    kw['datetimekeys4sort']={'trip_history':['mixed_start_datetime','start_date','started_at'],\\\n",
    "                                 'past_outages':'end','current_outages':'start',\\\n",
    "                                 'weather':'datetime','holidays':'date'}\n",
    "    kw['raw_date_formats']={'trip_history':{f:date_format_standard for f in\\\n",
    "                    ['mixed_start_datetime','start_date','end_date','started_at','ended_at']},\\\n",
    "            'past_outages':{'start':date_format_standard,'end':date_format_standard},\n",
    "            'current_outages':{'timestamp_now':date_format_standard,'start':date_format_standard},\\\n",
    "            'weather':{'datetime':'%m/%d/%Y %H:%M:%S'},\n",
    "            'holidays':{'date':None}}\n",
    "    if_exists = {'holidays':'append','current_outages':'replace'}  # default is 'append' if unspecified\n",
    "    #paid_tables = ['weather']\n",
    "    #raw_date_formats = {'weather':'%m/%d/%Y %H:%M:%S'}  # default: date_format_standard\n",
    "    if (now is None):\n",
    "        kw['now']=datetime.datetime.now()\n",
    "    else:\n",
    "        assert isinstance(now,datetime.datetime), \"'now' argument must be an instance of type datetime.datetime\"\n",
    "        kw['now']=now\n",
    "    globs = globals()\n",
    "    \n",
    "    for (t,table_name) in tables.items():\n",
    "        datecol = kw['datetimekeys4sort'][t]\n",
    "        datefields_rawformats = kw['raw_date_formats'][t]\n",
    "        mrt = most_recent_timestamp(db,table_name,datecol)\n",
    "        fun_name = 'read_'+t\n",
    "        df_new = globs[fun_name](db,table_name,mrt,kw) # get new data\n",
    "        if (len(df_new)==0):\n",
    "            continue\n",
    "        # 1) Standardize field names\n",
    "        col_renames = {c:c.lower().replace(\" \",\"_\") for c in df_new.columns}\n",
    "        df_new = df_new.rename(columns=col_renames) # standardize fieldname format\n",
    "        # 2) Collect datetimekeys4sort in one column, if necessary\n",
    "        if (isinstance(datecol,list)):\n",
    "            # date format is not consistent for this source\n",
    "            # copy into datecol[0] so that a single column holds all dates for sorting\n",
    "            (target_col,candidate_cols)=(datecol[0],[c for c in datecol[1:] if c in df_new.columns])\n",
    "            datecol=datecol[0]\n",
    "            if (target_col not in df_new.columns):\n",
    "                df_new = pd.concat([pd.Series(\\\n",
    "                            cabi_start_date.strftime(kw['raw_date_formats'][t][datecol]),\\\n",
    "                                              name=datecol,index=df_new.index),df_new],axis=1)\n",
    "            for c in candidate_cols:\n",
    "                df_new[target_col].update(df_new[c])\n",
    "        # 3) Standardize all date formats\n",
    "        for dfield in datefields_rawformats: # standardize all date formats\n",
    "            if dfield in df_new.columns:\n",
    "                tic=time.time()\n",
    "                print('Converting date format for %s, field %s ...' % (t,dfield))\n",
    "                if isinstance(df_new[dfield].iloc[0],datetime.date):\n",
    "                    # only holidays include date objects, all other sources provide strings\n",
    "                    df_new[dfield] = df_new[dfield].astype(str)\n",
    "                    # hack... but holidays will remain short strings from datetime.date() not datetime.datetime()\n",
    "                else:\n",
    "                    # all data sources other than holidays:\n",
    "                    df_new[dfield] = pd.to_datetime(df_new[dfield],format=datefields_rawformats[dfield])\\\n",
    "                                            .dt.strftime(date_format_standard)\n",
    "                print(time.time()-tic)\n",
    "        # 4) Sort and/or dedupe rows:        \n",
    "        tic=time.time()\n",
    "        print('Sorting / deduping rows, %s ... ' % t)\n",
    "        df_new = df_new.sort_values(datecol)\\\n",
    "                        .reset_index(drop=True)\n",
    "        dfndc = df_new[datecol]\n",
    "        startrow=0\n",
    "        mrt_str = mrt.strftime(date_format_standard)\n",
    "        while ((startrow<len(df_new)) and \\\n",
    "                       (dfndc.iloc[startrow] <= mrt_str)):\n",
    "            # ++: compare hashes here to allow insertion if sources evolve historical data?\n",
    "            startrow+=1\n",
    "        print(time.time()-tic)\n",
    "        # 5) Write to DB:\n",
    "        tic=time.time()\n",
    "        print('Writing table %s to database %s ...' % (table_name,db))\n",
    "        if (if_exists.get(t,'append')=='replace'): # always write entire table if 'replace'\n",
    "            startrow=0\n",
    "        if (startrow<len(df_new)):\n",
    "            conn=sqlite3.connect(db)\n",
    "            try:\n",
    "                df_new.iloc[startrow:].to_sql(table_name,conn,if_exists=if_exists.get(t,'append'),index=False)\n",
    "                conn.close()\n",
    "            except:\n",
    "                conn.close()\n",
    "                print('    Write failed. Instead writing to a backup json file')\n",
    "                df_new.iloc[startrow:].to_json(path_or_buf=t+'_backup_'+\\\n",
    "                           datetime.datetime.now().strftime(date_format_standard)+'.json')\n",
    "        print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cabi_wrangle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "db7 = 'cabi7.db'\n",
    "nowTime = datetime.datetime(2021,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file 2010-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2011-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2012-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2013-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2014-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2015-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2016-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 2017-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201801-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201802-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201803-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201804-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201805-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201806-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201807-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201808-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201809-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201810-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201811-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201812-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201901-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201902-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201903-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201904-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201905-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201906-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201907-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201908-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201909-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201910-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201911-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 201912-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202001-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202002-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202003-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202004-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202005-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202006-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202007-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202008-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202009-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202010-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202011-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202012-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file 202101-capitalbikeshare-tripdata.zip, which has presumably been read previously\n",
      "Skipping file index.html because it is not a zip\n",
      "Retrieving cabitracker outage data between 2021-02-17 and 2021-02-17 ...\n",
      "Converting date format for past_outages, field start ...\n",
      "0.021008014678955078\n",
      "Converting date format for past_outages, field end ...\n",
      "0.008986234664916992\n",
      "Sorting / deduping rows, past_outages ... \n",
      "0.002000093460083008\n",
      "Writing table past_outages to database cabi7.db ...\n",
      "0.013997554779052734\n",
      "Reading Current Outage data from CabiTracker...\n",
      "Execution Time(s): 2.18\n",
      "Converting date format for current_outages, field timestamp_now ...\n",
      "0.0009999275207519531\n",
      "Converting date format for current_outages, field start ...\n",
      "0.0009999275207519531\n",
      "Sorting / deduping rows, current_outages ... \n",
      "0.0010001659393310547\n",
      "Writing table current_outages to database cabi7.db ...\n",
      "0.05701494216918945\n",
      "2020-12-31 23:30:00\n",
      "2021-01-01 00:00:00\n",
      "2021-01-01 00:00:00\n",
      "https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/weatherdata/history?goal=history&startDateTime=2021-01-01T00:00:00&endDateTime=2021-01-01T00:00\n",
      "Reading new Weather Data from Visual Crossing...\n",
      "Execution Time(s): 0.29\n",
      "Converting date format for weather, field datetime ...\n",
      "0.002000093460083008\n",
      "Sorting / deduping rows, weather ... \n",
      "0.001018524169921875\n",
      "Writing table weather to database cabi7.db ...\n",
      "0.015981197357177734\n",
      "SQL Query did not work, returning cabi_start_date as most_recent\n",
      "Converting date format for holidays, field date ...\n",
      "0.0010001659393310547\n",
      "Sorting / deduping rows, holidays ... \n",
      "0.0010001659393310547\n",
      "Writing table holidays to database cabi7.db ...\n",
      "0.012015581130981445\n"
     ]
    }
   ],
   "source": [
    "read_data_update(db7,now=nowTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Station Information from GBFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "uri_stainfo = r'https://gbfs.capitalbikeshare.com/gbfs/en/station_information.json'\n",
    "stainfo = pd.DataFrame(json.loads((requests.get(uri_stainfo).content))['data']['stations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(623, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stainfo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eightd_has_key_dispenser</th>\n",
       "      <th>short_name</th>\n",
       "      <th>name</th>\n",
       "      <th>lon</th>\n",
       "      <th>rental_methods</th>\n",
       "      <th>rental_uris</th>\n",
       "      <th>has_kiosk</th>\n",
       "      <th>electric_bike_surcharge_waiver</th>\n",
       "      <th>external_id</th>\n",
       "      <th>legacy_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>station_type</th>\n",
       "      <th>eightd_station_services</th>\n",
       "      <th>capacity</th>\n",
       "      <th>station_id</th>\n",
       "      <th>region_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>31000</td>\n",
       "      <td>Eads St &amp; 15th St S</td>\n",
       "      <td>-77.053230</td>\n",
       "      <td>[KEY, CREDITCARD]</td>\n",
       "      <td>{'android': 'https://dc.lft.to/lastmile_qr_sca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>082469cc-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>1</td>\n",
       "      <td>38.858971</td>\n",
       "      <td>classic</td>\n",
       "      <td>[]</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>31001</td>\n",
       "      <td>18th St &amp; S Eads St</td>\n",
       "      <td>-77.053320</td>\n",
       "      <td>[KEY, CREDITCARD]</td>\n",
       "      <td>{'android': 'https://dc.lft.to/lastmile_qr_sca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>08246b69-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>2</td>\n",
       "      <td>38.857250</td>\n",
       "      <td>classic</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>31002</td>\n",
       "      <td>Crystal Dr &amp; 20th St S</td>\n",
       "      <td>-77.049232</td>\n",
       "      <td>[KEY, CREDITCARD]</td>\n",
       "      <td>{'android': 'https://dc.lft.to/lastmile_qr_sca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>08246c35-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>3</td>\n",
       "      <td>38.856425</td>\n",
       "      <td>classic</td>\n",
       "      <td>[]</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>31003</td>\n",
       "      <td>Crystal Dr &amp; 15th St S</td>\n",
       "      <td>-77.049417</td>\n",
       "      <td>[KEY, CREDITCARD]</td>\n",
       "      <td>{'android': 'https://dc.lft.to/lastmile_qr_sca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>08246cd5-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>4</td>\n",
       "      <td>38.861056</td>\n",
       "      <td>classic</td>\n",
       "      <td>[]</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>31004</td>\n",
       "      <td>Aurora Hills Cmty Ctr / 18th St &amp; S Hayes St</td>\n",
       "      <td>-77.059490</td>\n",
       "      <td>[KEY, CREDITCARD]</td>\n",
       "      <td>{'android': 'https://dc.lft.to/lastmile_qr_sca...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>08246d68-1f3f-11e7-bf6b-3863bb334450</td>\n",
       "      <td>5</td>\n",
       "      <td>38.857866</td>\n",
       "      <td>classic</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eightd_has_key_dispenser short_name  \\\n",
       "0                     False      31000   \n",
       "1                     False      31001   \n",
       "2                     False      31002   \n",
       "3                     False      31003   \n",
       "4                     False      31004   \n",
       "\n",
       "                                           name        lon     rental_methods  \\\n",
       "0                           Eads St & 15th St S -77.053230  [KEY, CREDITCARD]   \n",
       "1                           18th St & S Eads St -77.053320  [KEY, CREDITCARD]   \n",
       "2                        Crystal Dr & 20th St S -77.049232  [KEY, CREDITCARD]   \n",
       "3                        Crystal Dr & 15th St S -77.049417  [KEY, CREDITCARD]   \n",
       "4  Aurora Hills Cmty Ctr / 18th St & S Hayes St -77.059490  [KEY, CREDITCARD]   \n",
       "\n",
       "                                         rental_uris  has_kiosk  \\\n",
       "0  {'android': 'https://dc.lft.to/lastmile_qr_sca...       True   \n",
       "1  {'android': 'https://dc.lft.to/lastmile_qr_sca...       True   \n",
       "2  {'android': 'https://dc.lft.to/lastmile_qr_sca...       True   \n",
       "3  {'android': 'https://dc.lft.to/lastmile_qr_sca...       True   \n",
       "4  {'android': 'https://dc.lft.to/lastmile_qr_sca...       True   \n",
       "\n",
       "   electric_bike_surcharge_waiver                           external_id  \\\n",
       "0                           False  082469cc-1f3f-11e7-bf6b-3863bb334450   \n",
       "1                           False  08246b69-1f3f-11e7-bf6b-3863bb334450   \n",
       "2                           False  08246c35-1f3f-11e7-bf6b-3863bb334450   \n",
       "3                           False  08246cd5-1f3f-11e7-bf6b-3863bb334450   \n",
       "4                           False  08246d68-1f3f-11e7-bf6b-3863bb334450   \n",
       "\n",
       "  legacy_id        lat station_type eightd_station_services  capacity  \\\n",
       "0         1  38.858971      classic                      []        15   \n",
       "1         2  38.857250      classic                      []        11   \n",
       "2         3  38.856425      classic                      []        17   \n",
       "3         4  38.861056      classic                      []        16   \n",
       "4         5  38.857866      classic                      []        11   \n",
       "\n",
       "  station_id region_id  \n",
       "0          1        41  \n",
       "1          2        41  \n",
       "2          3        41  \n",
       "3          4        41  \n",
       "4          5        41  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stainfo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
